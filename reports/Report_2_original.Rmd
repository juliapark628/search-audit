---
title: 'Search Audit Report: Northern Florida Eviction Queries'
date: "10/28/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidytext)
library(stringr)

# get the ultimate directory
dir_files <- "/Users/juliapark/GitHub/Lab_Legal_Design/search_audit/data"

# remove queries that are irrelevant to the scenario
remove_queries <- 
  c(
    "couple's counseling bay area", 
    "emergency phone number", 
    "peacekeeper solutions", 
    "Places I can stay", 
    "how to make good relationship with partner", 
    "relationship advice and coaching", 
    "local police department", 
    "defusing a situation", 
    "ptsd", 
    "How to run away", 
    "Spouse/husband said he would hurt me", 
    "escaping relationship", 
    "911", 
    "police number", 
    "paranoia", 
    "Places that are safe", 
    "Best methods for good marriage life", 
    "where to go for advice with a troublesome relationship", 
    "I need help", 
    "support group locations", 
    "insecurity and anger", 
    "How to contact the police", 
    "Problems and solutions with wife",
    "bad roofing company",
    "free legal aid",
    "husband",
    "lawsuit help"
  )
```


```{r, echo=FALSE, message=FALSE}
# get directory of all files
files <- fs::dir_ls(dir_files)

# read in all data into one large table
all_tables <- 
  files %>%
  map_dfr(read_csv, .id = "legal_type") %>% 
  mutate(
    # get legal_type from file name
    legal_type = str_extract(legal_type, pattern = "(?<=result_).*") %>% str_remove("_all"), # my files don't have .csv, but if they do add (?=\\.csv) after asterisks
    place = str_extract(location, pattern = "[^,]+") %>% str_to_title(),
    place = case_when( # rename zip code into cities
      place %in% "96813" ~ "Honolulu",
      place %in% "96792" ~ "Rural Oahu",
      TRUE ~ place
    ),
    state = case_when( # get State information
      place %in% c("Honolulu", "Maui", "Rural Oahu", "Rural Hilo") ~ "Hawaii", 
      place %in% c("Jacksonville", "Pensacola", "Tallahassee") ~ "Florida",
      TRUE ~ NA_character_
    ),
    full_url = str_c(urldomain, ".", urlsuffix)
  ) %>% 
  distinct() %>% 
  filter(!(qry %in% remove_queries)) 


# analyze all queries using this function
# gets the top 3 URL domains
# first page URL domains
# most common suffixes
# percentage ads
# most common ad domain
analysis <- function(loc, tp) {
  print(str_glue("Location: {loc}, Legal query: {tp}", loc = loc, tp = tp))
  
  sub_table <-
    all_tables %>% 
    filter(place %in% loc, legal_type == tp) %>% 
    select(place, legal_type, everything())
  
  print("Top 3 URL domains")
  sub_table %>% 
    filter(type == "general", sub_rank >= 3, !is.na(full_url)) %>%
    count(full_url, sort = TRUE) %>% 
    print()
  
  print("First page top URL domains")
  sub_table %>% 
    filter(!is.na(full_url)) %>% 
    count(full_url, sort = TRUE) %>% 
    print()
  
  print("Most common suffixes")
  sub_table %>% 
    filter(!is.na(urlsuffix)) %>% 
    count(urlsuffix, sort = TRUE) %>% 
    print()
  
  print("Percentage of ads")
  sub_table %>% 
    filter(type %in% c("ad", "general")) %>% 
    count(type) %>% 
    mutate(prop = n / sum(n) * 100) %>% 
    filter(type == "ad") %>% 
    pull(prop) %>% 
    print()
  
  print("Most common ad domain")
  sub_table %>% 
    filter(type == "ad") %>% 
    count(urldomain, sort = TRUE) 
}


# all_loc_type <- 
#   files %>% 
#   str_split_fixed("_", 3) %>% 
#   as_tibble(.name_repair = "unique") %>% 
#   rename(loc = `...1`, result = `...2`, tp = `...3`) %>% 
#   mutate(
#     loc = str_extract(loc, "(?<=../data/).*"),
#     loc = case_when(
#       loc %in% "96813" ~ "Honolulu",
#       loc %in% "96793" ~ "Maui",
#       loc %in% "96792" ~ "Rural Oahu",
#       loc %in% "96778" ~ "Rual Hilo",
#       TRUE ~ loc
#     ),
#     tp = str_extract(tp, ".*(?=\\.csv)")
#   ) %>% 
#   select(-result) 
#   
# Code to run all analysis; unfortunately formatting is less clear between cities
# all_loc_type %>% 
#   pmap(analysis)

``` 


## Breakdown of queries

Distinct types: 

```{r}
"/Users/juliapark/GitHub/Lab_Legal_Design/search_audit/data/Pensacola_result_eviction" %>% 
  read_csv() %>% 
  count(type)
```

Number of distinct queries:

```{r, echo=FALSE}
# check distinct queries 
all_tables %>% 
  distinct(qry, legal_type, place) %>% 
  count(legal_type, place)
```

## Comparison between different categories

```{r}
all_tables %>% 
  filter(type == "ad")
```


```{r, echo=FALSE}
# Domain suffix count by legal type

all_tables %>% 
  filter(!is.na(urlsuffix)) %>% 
  mutate(urlsuffix = fct_lump(urlsuffix, prop = 0.05)) %>% 
  count(urlsuffix, legal_type, sort = TRUE) %>%
  mutate(
    legal_type = str_replace(legal_type, "_", " ") %>% str_to_title(),
    urlsuffix = fct_reorder(urlsuffix, n) %>% fct_rev()
  ) %>% 
  ggplot(aes(urlsuffix, n, fill = legal_type)) +
  geom_col(position = "dodge") +
  theme_minimal() +
  labs(
    title = "Domain suffix count by legal type",
    subtitle = "Domestic violence query has more .org links, all others have more .com links",
    y = "Count",
    x = "Domain suffix",
    fill = "Legal type"
  )

# Domain suffix percent breakdown
all_tables %>% 
  filter(!is.na(urlsuffix)) %>% 
  mutate(urlsuffix = fct_lump(urlsuffix, prop = 0.05)) %>% 
  count(state, legal_type, urlsuffix) %>%
  group_by(state, legal_type) %>% 
  mutate(perc = n / sum(n)) %>% 
  ungroup() %>% 
  mutate(
    legal_type = str_replace(legal_type, "_", " ") %>% str_to_title(),
    urlsuffix = fct_reorder(urlsuffix, n) %>% fct_rev()
  ) %>% 
  ggplot(aes(urlsuffix, perc, fill = legal_type)) +
  geom_col(position = "dodge") +
  facet_grid(cols = vars(state)) +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Domain suffix percent breakdown",
    subtitle = str_glue(
      "Domestic violence query has more .org links, all others have more .com links",
      "\nSpread between FL and HI are similar across all search query types"
    ),
    y = "Percent",
    x = "Domain suffix",
    fill = "Legal type"
  )

# Percent of ads on a given search page
all_tables %>% 
  filter(type %in% c("ad", "general")) %>% 
  count(type, legal_type) %>% 
  group_by(legal_type) %>% 
  mutate(prop = n / sum(n)) %>% 
  filter(type == "ad") %>% 
  ungroup() %>% 
  mutate(
    legal_type = str_replace(legal_type, "_", " ") %>% str_to_title(),
    legal_type = fct_reorder(legal_type, prop) %>% fct_rev()
  ) %>% 
  ggplot(aes(legal_type, prop)) +
  geom_col() +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Percent of ads on a given search page", 
    subtitle = str_glue(
      "Domestic violence has the largest proportion of ads, eviction queries have the least"
    ),
    x = "Legal query type",
    y = "Percent"
  ) 


# extract non-US websites, find the % of websites outside of US
all_tables %>% 
  mutate(non_us = str_extract(urlsuffix, "[^\\.]+$") %in% c("uk", "sg", "nz", "jp", "ca", "au")) %>% 
  count(legal_type, non_us) %>% 
  group_by(legal_type) %>% 
  mutate(perc = n / sum(n)) %>% 
  ungroup() %>% 
  filter(non_us == TRUE) %>% 
  mutate(
    legal_type = str_replace(legal_type, "_", "\n") %>% str_to_title(),
    legal_type = fct_reorder(legal_type, perc) %>% fct_rev()
  ) %>% 
  ggplot(aes(legal_type, perc)) +
  geom_col() +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  labs(
    title = "Percent of non-US domain names", 
    subtitle = str_glue(
      "Domestic violence has >1% links leading to non-US sites in the first Google search result page",
      "\n from Australia, UK, Canada, Singapore, and New Zealand"
    ),
    x = "Legal query type",
    y = "Percent of non-US domain names"
  ) 

all_tables %>% 
  mutate(non_us = str_extract(urlsuffix, "[^\\.]+$") %in% c("uk", "sg", "nz", "jp", "ca", "au")) %>% 
  count(state, legal_type, non_us) %>% 
  group_by(state, legal_type) %>% 
  mutate(perc = n / sum(n)) %>% 
  ungroup() %>% 
  filter(non_us == TRUE) %>% 
  mutate(
    legal_type = str_replace(legal_type, "_", "\n") %>% str_to_title(),
    legal_type = fct_reorder(legal_type, perc) %>% fct_rev()
  ) %>% 
  ggplot(aes(legal_type, perc)) +
  geom_col() +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  facet_grid(cols = vars(state)) +
  labs(
    title = "Percent of non-US domain names", 
    subtitle = str_glue(
      "Domestic violence has >1% links leading to non-US sites in the first Google search result page",
      "\n from Australia, UK, Canada, Singapore, and New Zealand"
    ),
    x = "Legal query type",
    y = "Percent of non-US domain names"
  ) 

```

## Rural Oahu

### Debt collection

```{r, echo=FALSE}
analysis("Rural Oahu", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Rural Oahu", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Rural Oahu", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Rural Oahu", "eviction")
```

## Honolulu

### Debt collection

```{r, echo=FALSE}
analysis("Honolulu", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Honolulu", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Honolulu", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Honolulu", "eviction")
```

## Jacksonville

### Debt collection

```{r, echo=FALSE}
analysis("Jacksonville", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Jacksonville", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Jacksonville", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Jacksonville", "eviction")
```

## Pensacola

### Debt collection

```{r, echo=FALSE}
analysis("Pensacola", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Pensacola", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Pensacola", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Pensacola", "eviction")
```

## Tallahassee

### Debt collection

```{r, echo=FALSE}
analysis("Tallahassee", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Tallahassee", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Tallahassee", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Tallahassee", "eviction")
```



```{r, eval=FALSE, echo=FALSE}
# Write to separate tables to include thhe most common URLs and links

all_tables %>% 
  filter(!is.na(full_url)) %>% 
  count(full_url, sort=TRUE) %>% 
  write_csv("most_common_url.csv")

all_tables %>% 
  filter(!is.na(url)) %>% 
  count(url, sort = TRUE) %>% 
  write_csv("most_common_links.csv")

all_tables %>% 
  write_csv("all_queries.csv")
```

