---
title: 'Search Audit Report: Florida/Hawaii Search Queries: Nov. 2020'
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "11/15/2020"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(tidytext)
library(stringr)

# get the ultimate directory
dir_files <- "/Users/juliapark/GitHub/Lab_Legal_Design/search_audit/data/FL_HI_audit_Nov_2020"
all_queries_jan_2020_filename <- "/Users/juliapark/GitHub/Lab_Legal_Design/search_audit/data/FL_HI_audit_Jan_2020/all_queries.csv"

# remove queries that are irrelevant to the scenario
remove_queries <- 
  c(
    "couple's counseling bay area", 
    "emergency phone number", 
    "peacekeeper solutions", 
    "Places I can stay", 
    "how to make good relationship with partner", 
    "relationship advice and coaching", 
    "local police department", 
    "defusing a situation", 
    "ptsd", 
    "How to run away", 
    "Spouse/husband said he would hurt me", 
    "escaping relationship", 
    "911", 
    "police number", 
    "paranoia", 
    "Places that are safe", 
    "Best methods for good marriage life", 
    "where to go for advice with a troublesome relationship", 
    "I need help", 
    "support group locations", 
    "insecurity and anger", 
    "How to contact the police", 
    "Problems and solutions with wife",
    "bad roofing company",
    "free legal aid",
    "husband",
    "lawsuit help"
  )
```

```{r read previousdata, echo=FALSE, message=FALSE}
all_queries_jan_2020 <- 
  all_queries_jan_2020_filename %>% 
  read_csv() %>% 
  filter(!str_detect(qry, "california")) %>% 
  filter(!str_detect(qry, "califronia")) %>% 
  filter(!str_detect(qry, "ca")) %>% 
  filter(!str_detect(qry, "san francisco")) %>% 
  filter(!str_detect(qry, "sf")) %>% 
  filter(!str_detect(qry, "bay area"))
```

```{r read current data, echo=FALSE, message=FALSE}
# get directory of all files
files <- fs::dir_ls(dir_files)

# read in all data into one large table
all_tables <- 
  files %>%
  map_dfr(read_csv, .id = "legal_type") %>% 
  mutate(
    # get legal_type from file name
    legal_type = str_extract(legal_type, pattern = "(?<=result_).*") %>% str_remove("_all"), # my files don't have .csv, but if they do add (?=\\.csv) after asterisks
    place = str_extract(location, pattern = "[^,]+") %>% str_to_title(),
    place = case_when( # rename zip code into cities
      place %in% "96813" ~ "Honolulu",
      place %in% "96792" ~ "Rural Oahu",
      TRUE ~ place
    ),
    state = case_when( # get State information
      place %in% c("Honolulu", "Maui", "Rural Oahu", "Rural Hilo") ~ "Hawaii", 
      place %in% c("Jacksonville", "Pensacola", "Tallahassee") ~ "Florida",
      TRUE ~ NA_character_
    ),
    full_url = str_c(urldomain, ".", urlsuffix)
  ) %>% 
  distinct() %>% 
  filter(!(qry %in% remove_queries))


all_tables <- all_tables %>% 
  filter(!str_detect(qry, "Hawaii")) %>% 
  filter(!str_detect(qry, "Florida")) %>% 
  filter(!str_detect(qry, "Honolulu")) %>% 
  filter(!str_detect(qry, "Oahu")) %>% 
  filter(!str_detect(qry, "Jacksonville")) %>% 
  filter(!str_detect(qry, "Pensacola")) %>% 
  filter(!str_detect(qry, "Tallahassee"))
```

```{r save data in files, eval=FALSE, echo=FALSE}
# Write to separate tables to include thhe most common URLs and links

# all_tables %>% 
#   filter(!is.na(full_url)) %>% 
#   count(full_url, sort=TRUE) %>% 
#   write_csv("most_common_url_complete.csv")
# 
# all_tables %>% 
#   filter(!is.na(url)) %>% 
#   count(url, sort = TRUE) %>% 
#   write_csv("most_common_links_complete.csv")
# 
# all_tables %>% 
#   write_csv("all_queries_complete.csv")
```

```{r analysis function, echo=FALSE, message=FALSE}
# analyze all queries using this function
# gets the top 3 URL domains
# first page URL domains
# most common suffixes
# percentage ads
# most common ad domain
analysis <- function(loc, tp) {
  print(str_glue("Location: {loc}, Legal query: {tp}", loc = loc, tp = tp))
  
  sub_table <-
    all_tables %>% 
    filter(place %in% loc, legal_type == tp) %>% 
    select(place, legal_type, everything())
  
  print("Top 3 URL domains")
  sub_table %>% 
    filter(type == "general", sub_rank >= 3, !is.na(full_url)) %>%
    count(full_url, sort = TRUE) %>% 
    print()
  
  print("First page top URL domains")
  sub_table %>% 
    filter(!is.na(full_url)) %>% 
    count(full_url, sort = TRUE) %>% 
    print()
  
  print("Most common suffixes")
  sub_table %>% 
    filter(!is.na(urlsuffix)) %>% 
    count(urlsuffix, sort = TRUE) %>% 
    print()
  
  print("Percentage of ads")
  sub_table %>% 
    filter(type %in% c("ad", "general")) %>% 
    count(type) %>% 
    mutate(prop = n / sum(n) * 100) %>% 
    filter(type == "ad") %>% 
    pull(prop) %>% 
    print()
  
  print("Most common ad domain")
  sub_table %>% 
    filter(type == "ad") %>% 
    count(urldomain, sort = TRUE) 
}


# all_loc_type <- 
#   files %>% 
#   str_split_fixed("_", 3) %>% 
#   as_tibble(.name_repair = "unique") %>% 
#   rename(loc = `...1`, result = `...2`, tp = `...3`) %>% 
#   mutate(
#     loc = str_extract(loc, "(?<=../data/).*"),
#     loc = case_when(
#       loc %in% "96813" ~ "Honolulu",
#       loc %in% "96793" ~ "Maui",
#       loc %in% "96792" ~ "Rural Oahu",
#       loc %in% "96778" ~ "Rual Hilo",
#       TRUE ~ loc
#     ),
#     tp = str_extract(tp, ".*(?=\\.csv)")
#   ) %>% 
#   select(-result) 
#   
# Code to run all analysis; unfortunately formatting is less clear between cities
# all_loc_type %>% 
#   pmap(analysis)

``` 



## Comparison between different categories

```{r}
all_tables %>% 
  filter(type == "ad")
```

```{r create suffix table}

#mutate(urlsuffix = fct_lump(urlsuffix, prop = 0.05)) %>% 
suffix_nov_2020 <-
  all_tables %>% 
  filter(!is.na(urlsuffix)) %>% 
  mutate(
    urlsuffix = case_when(
      urlsuffix == "com" ~ "com",
      urlsuffix == "org" ~ "org", 
      urlsuffix == "gov" ~ "gov", 
      TRUE ~ "other"
    )
  ) %>% 
  count(state, legal_type, urlsuffix) %>%
  group_by(state, legal_type) %>% 
  mutate(perc = n / sum(n)) %>% 
  ungroup() %>% 
  mutate(
    legal_type = str_replace(legal_type, "_", " ") %>% str_to_title(),
    urlsuffix = fct_reorder(urlsuffix, n) %>% fct_rev(), 
    time = "November"
  ) 

suffix_jan_2020 <-
  all_queries_jan_2020 %>% 
  filter(!is.na(urlsuffix)) %>% 
  mutate(
    urlsuffix = case_when(
      urlsuffix == "com" ~ "com",
      urlsuffix == "org" ~ "org", 
      urlsuffix == "gov" ~ "gov", 
      TRUE ~ "other"
    )
  ) %>% 
  count(state, legal_type, urlsuffix) %>%
  group_by(state, legal_type) %>% 
  mutate(perc = n / sum(n)) %>% 
  ungroup() %>% 
  mutate(
    legal_type = str_replace(legal_type, "_", " ") %>% str_to_title(),
    urlsuffix = fct_reorder(urlsuffix, n) %>% fct_rev(), 
    time = "January"
  )

suffix <- rbind(suffix_jan_2020, suffix_nov_2020)
```

```{r create suffix graphs}
suffix %>% 
  filter(state == "Florida") %>% 
  ggplot(aes(factor(time), perc, color = urlsuffix, group = legal_type)) +
  geom_point() + 
  scale_y_continuous(
    breaks = scales::breaks_width(0.1),
    minor_breaks = NULL,
    labels = scales::label_percent(accuracy = 1)
  ) + 
  facet_grid(cols = vars(legal_type)) + 
  theme_minimal()  +
  labs(
    title = "Florida: Domain suffix percent breakdown",
    subtitle = str_glue(
      "Share of .org links decreases for all legal types, but .gov increases",
      "\n.com links increase for debt collection and domestic violence"
    ),
    y = "Percent",
    x = "Time",
    color = "Suffix"
  )


suffix %>% 
  filter(state == "Hawaii") %>% 
  ggplot(aes(factor(time), perc, color = urlsuffix, group = legal_type)) +
  geom_point() + 
  scale_y_continuous(
    breaks = scales::breaks_width(0.1),
    minor_breaks = NULL,
    labels = scales::label_percent(accuracy = 1)
  ) + 
  facet_grid(cols = vars(legal_type)) + 
  theme_minimal() +
  labs(
    title = "Hawaii: Domain suffix percent breakdown",
    subtitle = "Very similar trend to Florida",
    y = "Percent",
    x = "Time",
    color = "Legal type"
  )
```


```{r create nonUS tables}
nonUS_nov_2020 <-
  all_tables %>% 
  mutate(non_us = str_extract(urlsuffix, "[^\\.]+$") %in% c("uk", "sg", "nz", "jp", "ca", "au")) %>% 
  count(state, legal_type, non_us) %>% 
  group_by(state, legal_type) %>% 
  mutate(perc = n / sum(n)) %>% 
  ungroup() %>% 
  filter(non_us == TRUE) %>% 
  mutate(
    legal_type = str_replace(legal_type, "_", "\n") %>% str_to_title(),
    legal_type = fct_reorder(legal_type, perc) %>% fct_rev(), 
    time = "November"
  )

nonUS_jan_2020 <-
  all_tables %>% 
  mutate(non_us = str_extract(urlsuffix, "[^\\.]+$") %in% c("uk", "sg", "nz", "jp", "ca", "au")) %>% 
  count(state, legal_type, non_us) %>% 
  group_by(state, legal_type) %>% 
  mutate(perc = n / sum(n)) %>% 
  ungroup() %>% 
  filter(non_us == TRUE) %>% 
  mutate(
    legal_type = str_replace(legal_type, "_", "\n") %>% str_to_title(),
    legal_type = fct_reorder(legal_type, perc) %>% fct_rev(), 
    time = "January"
  )

nonUS <- rbind(nonUS_jan_2020, nonUS_nov_2020)
```


```{r create nonUS graphs}
nonUS %>% 
  ggplot(aes(legal_type, perc, fill = time)) +
  geom_col(position = "dodge") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  facet_grid(cols = vars(state)) +
  labs(
    title = "Percent of non-US domain names", 
    subtitle = str_glue(
      "Proportion of nonUS domains nearly the same in November as in January"
    ),
    x = "Legal query type",
    y = "Percent of non-US domain names"
  ) 
```


```{r, echo=FALSE}
# Percent of ads on a given search page
# all_tables %>% 
#   filter(type %in% c("ad", "general")) %>% 
#   count(type, legal_type) %>% 
#   group_by(legal_type) %>% 
#   mutate(prop = n / sum(n)) %>% 
#   filter(type == "ad") %>% 
#   ungroup() %>% 
#   mutate(
#     legal_type = str_replace(legal_type, "_", " ") %>% str_to_title(),
#     legal_type = fct_reorder(legal_type, prop) %>% fct_rev()
#   ) %>% 
#   ggplot(aes(legal_type, prop)) +
#   geom_col() +
#   theme_minimal() +
#   scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
#   labs(
#     title = "Percent of ads on a given search page", 
#     subtitle = str_glue(
#       "Domestic violence has the largest proportion of ads, eviction queries have the least"
#     ),
#     x = "Legal query type",
#     y = "Percent"
#   ) 
```

## Rural Oahu

### Debt collection

```{r, echo=FALSE}
analysis("Rural Oahu", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Rural Oahu", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Rural Oahu", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Rural Oahu", "eviction")
```

## Honolulu

### Debt collection

```{r, echo=FALSE}
analysis("Honolulu", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Honolulu", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Honolulu", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Honolulu", "eviction")
```

## Jacksonville

### Debt collection

```{r, echo=FALSE}
analysis("Jacksonville", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Jacksonville", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Jacksonville", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Jacksonville", "eviction")
```

## Pensacola

### Debt collection

```{r, echo=FALSE}
analysis("Pensacola", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Pensacola", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Pensacola", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Pensacola", "eviction")
```

## Tallahassee

### Debt collection

```{r, echo=FALSE}
analysis("Tallahassee", "debt_collection")
```


### Domestic violence

```{r, echo=FALSE}
analysis("Tallahassee", "domestic_violence")
```


### Flood contractor

```{r, echo=FALSE}
analysis("Tallahassee", "flood_contractor")
```

### Eviction

```{r, echo=FALSE}
analysis("Tallahassee", "eviction")
```





